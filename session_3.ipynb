{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:2px;border-color:#094780\">\n",
    "<center><h1> Working with numbers and time series in Python </h1></center>\n",
    "<hr style=\"border-width:2px;border-color:#094780\">\n",
    "\n",
    "### Introduction\n",
    "In the world of finance, the ability to efficiently manipulate and analyze vast amounts of data is crucial. Financial data, whether it be stock prices, trading volumes, or economic indicators, often comes in large, complex datasets that need to be processed quickly and accurately. This is where Pandas and NumPy, two powerful Python libraries, come into play.\n",
    "\n",
    "**NumPy** provides the foundation for **numerical computing** in Python. Its support for large, multi-dimensional arrays and matrices, along with a rich collection of mathematical functions, allows for the efficient computation of complex financial models. Whether you're calculating returns, risks, or optimizing portfolios, NumPy enables these operations to be performed quickly and with minimal code.\n",
    "\n",
    "**Pandas**, on the other hand, is built on top of NumPy and is specifically designed for **data manipulation and analysis**. It introduces data structures like Series and DataFrames, which are ideal for handling tabular financial data. Pandas makes it easy to clean, filter, aggregate, and analyze time series data, which is a common task in finance. With Pandas, you can effortlessly transform raw data into actionable insights, enabling more informed decision-making.\n",
    "\n",
    "Together, Pandas and NumPy form the backbone of financial data analysis in Python, empowering finance professionals and engineers to tackle a wide range of challenges, from simple data exploration to building complex financial models.\n",
    "\n",
    "### Summary :\n",
    "\n",
    " - <a href=\"#C1\">Start by reading and writing files</a>\n",
    " - <a href=\"#C2\">Introduction to NumPy</a>\n",
    " - <a href=\"#C3\">Introduction to Pandas</a>\n",
    " - <a href=\"#C4\">Introduction to Polars</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"C1\">Start by reading and writing files</a>\n",
    "When working with financial data, you may encounter different file formats such as TXT, CSV, and JSON. Pythonâ€™s built-in libraries provide straightforward methods to read from and write to these files. \n",
    "\n",
    "Here is an example of reading a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file = open('financial_data.csv', 'r')\n",
    "reader = csv.reader(file)\n",
    "for row in reader:\n",
    "    print(row)  # Print each row as a list\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach, if an error occurs between opening and closing the file, the file may not be closed. The main problem with this is that each open file consumes a small amount of memory (RAM) for buffering and handling the file's data. If multiple files are left open, this can gradually increase memory usage. Therefore, it's more efficient to close the file even when errors occur in the code. Based on what we covered in the last session, we can use a `try-finally` clause to ensure this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Reading a CSV file\n",
    "file = open('financial_data.csv', 'r')\n",
    "try:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)  # Print each row as a list\n",
    "finally:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a simpler and more concise way to read a file than using a `try-finally` clause, and that is by using the `with` clause as shown below. Using the `with` clause ensures that the file is properly closed and prevents any human oversight in forgetting to close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Reading a CSV file\n",
    "with open('financial_data.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)  # Print each row as a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to compare opening file with and without 'with' clause\n",
    "\n",
    "import csv\n",
    "\n",
    "file = open('financial_data.csv', 'r')\n",
    "reader = csv.reader(file)\n",
    "for row in reader:\n",
    "    print(row)  # Print each row as a list\n",
    "raise\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file.closed)\n",
    "file.close()\n",
    "print(file.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Reading a CSV file\n",
    "with open('financial_data.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)  # Print each row as a list\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file.closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we can read files in Python, we can also create and write to new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Writing to a CSV file\n",
    "data = [['Date', 'Price', 'Volume'], ['2024-09-01', '100.50', '1500'], ['2024-09-02', '101.00', '1600']]\n",
    "with open('processed_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)  # Write multiple rows at once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"C2\">Introduction to NumPy</a>\n",
    "### **Importance of NumPy in Scientific Computing**\n",
    "\n",
    "NumPy (Numerical Python) is one of the core libraries in Python for numerical and scientific computing. Its importance comes from several key features that make it indispensable for handling large-scale scientific and engineering problems. \n",
    "\n",
    "#### 1. **Efficient Array Manipulation**\n",
    "At the heart of NumPy is the **ndarray** object, which allows for fast and efficient handling of multi-dimensional arrays (matrices, vectors). Unlike Python's built-in lists, which can store different data types and are slow for mathematical operations, NumPy arrays are **homogeneous** (all elements are of the same type) and optimized for performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create large list and NumPy array\n",
    "size = 10**6\n",
    "python_list = list(range(size))\n",
    "numpy_array = np.arange(size)\n",
    "\n",
    "# Time the addition for Python list\n",
    "start_time = time.time()\n",
    "python_list_result = [x + 1 for x in python_list]\n",
    "python_list_time = time.time() - start_time\n",
    "\n",
    "# Time the addition for NumPy array\n",
    "start_time = time.time()\n",
    "numpy_array_result = numpy_array + 1\n",
    "numpy_array_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Python list took: {python_list_time:.5f} seconds\")\n",
    "print(f\"NumPy array took: {numpy_array_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Create a large list and NumPy array\n",
    "size = 10**6\n",
    "python_list = list(range(size))\n",
    "numpy_array = np.arange(size)\n",
    "\n",
    "# Memory size of Python list\n",
    "python_list_memory = sys.getsizeof(python_list[0]) * len(python_list)\n",
    "\n",
    "# Memory size of NumPy array\n",
    "numpy_array_memory = numpy_array.nbytes\n",
    "\n",
    "# Print memory usage\n",
    "print(f\"Python list memory usage: {python_list_memory / 1024 / 1024:.5f} MB\")\n",
    "print(f\"NumPy array memory usage: {numpy_array_memory / 1024 / 1024:.5f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Vectorization and Broadcasting**\n",
    "NumPy's operations are **vectorized**, meaning you can apply mathematical functions to entire arrays without writing loops, resulting in cleaner, faster code. \n",
    "\n",
    "Additionally, NumPy supports **broadcasting**, a technique that automatically expands smaller arrays to match the size of larger ones during operations. This leads to more concise code and eliminates the need for manual resizing or complex loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorization's importance\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Large dataset of stock returns and weights\n",
    "n = 10**8\n",
    "\n",
    "returns = np.random.random(n)\n",
    "weights = np.random.random(n)\n",
    "\n",
    "# Non-vectorized loop approach\n",
    "start_time = time.time()\n",
    "portfolio_return = 0\n",
    "for i in range(n):\n",
    "    portfolio_return += returns[i] * weights[i]\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "# Vectorized NumPy approach\n",
    "start_time = time.time()\n",
    "portfolio_return_vectorized = np.dot(returns, weights)\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop approach took: {loop_time:.5f} seconds\")\n",
    "print(f\"Vectorized approach took: {vectorized_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Differences:**\n",
    "- **Performance:** NumPy optimizes this operation at a lower level (C code) making it much **faster** than manually looping through Python lists.\n",
    "- **Readability:** The vectorized code is **cleaner** and **easier to understand**. Instead of writing an explicit loop, you simply express the operation as a dot product between two arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The broadcasting's importance\n",
    "import numpy as np\n",
    "\n",
    "# Portfolio stock prices over 3 days (3x4 matrix for 4 stocks)\n",
    "prices = np.array([[100, 102, 98, 105],\n",
    "                   [101, 103, 99, 106],\n",
    "                   [102, 104, 100, 107]])\n",
    "\n",
    "# Percentage changes (1x4 vector)\n",
    "pct_changes = np.array([1.01, 1.02, 0.99, 1.03])\n",
    "\n",
    "# Broadcasting to apply the percentage changes to each day's prices\n",
    "new_prices = prices * pct_changes\n",
    "print(new_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A 2x3 matrix\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# A vector of shape (3,)\n",
    "vector = np.array([10, 20, 30])\n",
    "\n",
    "# Broadcasting the vector across the matrix rows\n",
    "result = matrix + vector\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. **Linear Algebra and Matrix Operations**\n",
    "Scientific computing heavily relies on **linear algebra**, and NumPy provides robust support for matrix operations, including:\n",
    "- **Matrix multiplication**.\n",
    "- **Dot products**.\n",
    "- **Eigenvalues and eigenvectors**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix inversion\n",
    "import numpy as np\n",
    "\n",
    "# Define a 2x2 matrix\n",
    "A = np.array([[4, 7],\n",
    "              [2, 6]])\n",
    "\n",
    "# Compute the inverse of the matrix\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Inverse of A:\\n\", A_inv)\n",
    "\n",
    "# Verify by multiplying the matrix with its inverse (should give identity matrix)\n",
    "I = A @ A_inv\n",
    "print(\"A @ A_inv (Identity Matrix):\\n\", np.round(I, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinant of a Matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define a 3x3 matrix\n",
    "B = np.array([[6, 1, 1],\n",
    "              [4, -2, 5],\n",
    "              [2, 8, 7]])\n",
    "\n",
    "# Compute the determinant\n",
    "det_B = np.linalg.det(B)\n",
    "\n",
    "print(\"Matrix B:\\n\", B)\n",
    "print(\"Determinant of B:\", det_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalues and Eigenvectors\n",
    "import numpy as np\n",
    "\n",
    "# Define a 2x2 matrix\n",
    "C = np.array([[4, -2],\n",
    "              [1, 1]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "\n",
    "print(\"Matrix C:\\n\", C)\n",
    "print(\"Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving a System of Linear Equations: A . x = b\n",
    "import numpy as np\n",
    "\n",
    "# Coefficient matrix A\n",
    "A = np.array([[3, 1], [1, 2]])\n",
    "\n",
    "# Constant vector b\n",
    "b = np.array([9, 8])\n",
    "\n",
    "# Solve the system A @ x = b\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"Solution to the system (x):\", x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4. **Wide Range of Mathematical Functions**\n",
    "NumPy provides an extensive set of mathematical functions, including:\n",
    "- **Trigonometric**, **logarithmic**, and **exponential** functions.\n",
    "- **Statistical** functions like mean, median, standard deviation, and variance.\n",
    "- **Fourier transforms** for signal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define angles in radians\n",
    "angles = np.array([0, np.pi/2, np.pi])\n",
    "\n",
    "# Compute sine, cosine, and tangent\n",
    "sin_values = np.sin(angles)\n",
    "cos_values = np.cos(angles)\n",
    "tan_values = np.tan(angles)\n",
    "\n",
    "print(\"Sine:\", sin_values)\n",
    "print(\"Cosine:\", cos_values)\n",
    "print(\"Tangent:\", tan_values)\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "# Exponential function\n",
    "exp_values = np.exp(x)\n",
    "print(\"Exponential:\", exp_values)  \n",
    "\n",
    "# Natural logarithm (log base e)\n",
    "log_values = np.log(x)\n",
    "print(\"Natural Log:\", log_values)  \n",
    "\n",
    "\n",
    "data = np.array([10, 20, 30, 40, 50])\n",
    "\n",
    "# Mean\n",
    "mean_value = np.mean(data)\n",
    "print(\"Mean:\", mean_value)\n",
    "\n",
    "# Median\n",
    "median_value = np.median(data)\n",
    "print(\"Median:\", median_value)\n",
    "\n",
    "# Standard Deviation\n",
    "std_deviation = np.std(data)\n",
    "print(\"Standard Deviation:\", std_deviation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "- Convert the following list to a numpy array:\n",
    "```python\n",
    "l = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "```\n",
    "\n",
    "- Use the result to extract a numpy array containing the even elements from arr.\n",
    "\n",
    "\n",
    "**Exercise 2**\n",
    "- Given the numpy array from exercise 1, reshape the array so that it becomes a 2 by 5 matrix.\n",
    "\n",
    "\n",
    "**Exercise 3**\n",
    "- Simulate a random uniform(0, 1) with 100 elements.\n",
    "- Simulate a random normal(0, 1) with 100 elements.\n",
    "\n",
    "\n",
    "**Exercise 4**\n",
    "Write a one-liner to compute the transpose of the following matrix:\n",
    "```python\n",
    "M = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "```\n",
    "\n",
    "\n",
    "**Exercise 5**\n",
    "\n",
    "- Given the following matrix M:\n",
    "```python\n",
    "M = np.array([[1,2,3], [2,1,4], [3,4,1]])\n",
    "```\n",
    "\n",
    "- Find the following matrix decomposition M = PDP^-1, where D is a diagonal matrix with the eigenvalues of M and P is an invertible (change of basis) matrix with columns the eigen vectors of M.\n",
    "\n",
    "- Perform the multiplication M = PDP^-1. Do you get the matrix M back ?\n",
    " \n",
    "- Display the dimensions of the martrix M\n",
    "\n",
    "\n",
    "**Exercise 6**\n",
    "- Read the data from linreg_data.csv using np.genfromtxt(\"linreg_data.csv\", delimiter=\",\", dtype=np.float64) and find the coefficients beta = [beta0, beta1] that minimizes ||y - AX||, where ||.|| is the l2 norm. Here, y is a vector of yi where (y-coordinates), A is an N by 2 matrix with ones in the first columns and xi in second column, and i goes from 1 to N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**\n",
    "\n",
    "**Implementing a Simple Monte Carlo Simulation for Option Pricing**\n",
    "\n",
    "The goal of this task is to implement a **Monte Carlo simulation** to estimate the price of a **European call option** using **NumPy**. A European call option gives the holder the right (but not the obligation) to buy an asset at a specified strike price $K$ on a specified expiration date $T$.\n",
    "\n",
    "In the Monte Carlo method, we simulate multiple possible future paths for the asset's price based on a stochastic model, typically the **Geometric Brownian Motion (GBM)** model, and compute the average payoff of the option over those simulated paths. The average payoff is then discounted to present value to estimate the option price.\n",
    "\n",
    "**Steps to Implement:**\n",
    "1. **Inputs**:\n",
    "   - Initial stock price $S_0$.\n",
    "   - Strike price $K$.\n",
    "   - Time to maturity $T$ (in years).\n",
    "   - Risk-free interest rate $r$.\n",
    "   - Volatility $\\sigma$ of the underlying asset.\n",
    "   - Number of simulations $N$.\n",
    "\n",
    "2. **Model**: The asset price follows **Geometric Brownian Motion (GBM)**, described by the following stochastic differential equation:\n",
    "\n",
    "   $$ dS = r S \\, dt + \\sigma S \\, dW $$\n",
    "\n",
    "   Where:\n",
    "   - $S$ is the asset price.\n",
    "   - $r$ is the risk-free rate.\n",
    "   - $\\sigma$ is the volatility of the asset.\n",
    "   - $dW$ represents the Wiener process (random component).\n",
    "\n",
    "3. **Monte Carlo Simulation**:\n",
    "   - Simulate $N$ different paths for the asset price at maturity $T$.\n",
    "   - Compute the payoff for each path: $ \\max(S_T - K, 0) $ for a call option.\n",
    "   - Average the payoffs and discount them to the present value using the risk-free rate.\n",
    "\n",
    "4. **Output**: The estimated price of the call option.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "The asset price at maturity $T$ can be simulated using:\n",
    "\n",
    "$$\n",
    "S_T = S_0 \\times \\exp \\left( (r - 0.5 \\sigma^2) T + \\sigma \\sqrt{T} Z \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Z$ is a random variable from the standard normal distribution $ Z \\sim N(0, 1) $.\n",
    "\n",
    "The payoff of the call option for each path is:\n",
    "\n",
    "$$\n",
    "\\text{Payoff} = \\max(S_T - K, 0)\n",
    "$$\n",
    "\n",
    "The option price is the present value of the average payoff:\n",
    "\n",
    "$$\n",
    "\\text{Option Price} = e^{-rT} \\times \\frac{1}{N} \\sum_{i=1}^{N} \\max(S_T^{(i)} - K, 0)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"C3\">Introduction to Pandas</a>\n",
    "Pandas is one of the most powerful and widely-used Python libraries for data analysis and manipulation, making it an essential tool in the world of finance. Its ability to handle large datasets efficiently and its rich set of functions for data manipulation makes it a key library for financial professionals who deal with time series, historical data, and complex financial computations.\n",
    "\n",
    "In Pandas, we have 2 fundamental data structures: **Series** and **dataframes**.\n",
    "\n",
    "\n",
    "A **Pandas Series** is a **one-dimensional** array-like object that can hold various data types, including integers, floats, strings, and more. Each element in a Series is associated with an index, similar to a dictionary in Python, where each value is mapped to a specific key.\n",
    "\n",
    "A **Pandas DataFrame** is a **two-dimensional** table-like structure where each column can be a different data type (similar to a spreadsheet or SQL table). It consists of rows and columns, with both row and column labels.\n",
    "\n",
    "\n",
    "| Feature           | **Series**                                      | **DataFrame**                                   |\n",
    "|-------------------|-------------------------------------------------|-------------------------------------------------|\n",
    "| **Dimensions**     | 1-dimensional (like a column)                   | 2-dimensional (like a table)                    |\n",
    "| **Indexing**       | Single index (for rows)                         | Dual index (row and column labels)              |\n",
    "| **Slicing**        | Label-based (`loc`), position-based (`iloc`)     | Label-based (`loc`), position-based (`iloc`)    |\n",
    "| **Structure**      | A labeled array or vector                       | A table of rows and columns                     |\n",
    "| **Operations**     | Element-wise operations supported               | Column-wise and row-wise operations supported   |\n",
    "| **Missing Data**   | Can handle missing data (`NaN`)                 | Can handle missing data (`NaN`)                 |\n",
    "| **Use Case**       | A single column of data (e.g., stock prices)     | A dataset with multiple variables (e.g., a financial report with multiple columns) |\n",
    "\n",
    "Here is the list of the main Pandas methods and functions:\n",
    "\n",
    "| **Category**                     | **Function/Method**         | **Description**                                    |\n",
    "|-----------------------------------|----------------------------|----------------------------------------------------|\n",
    "| **Data Creation and Loading**     | `pd.read_csv()`            | Load data from a CSV file.                         |\n",
    "|                                   | `pd.read_excel()`          | Load data from an Excel file.                      |\n",
    "|                                   | `pd.DataFrame()`           | Create a DataFrame from lists, dictionaries, etc.  |\n",
    "|                                   | `pd.Series()`              | Create a Pandas Series object.                     |\n",
    "| **Basic Data Exploration**        | `head()`                   | View the first rows of the DataFrame.              |\n",
    "|                                   | `tail()`                   | View the last rows of the DataFrame.               |\n",
    "|                                   | `info()`                   | Get a concise summary of the DataFrame.            |\n",
    "|                                   | `describe()`               | Generate descriptive statistics.                   |\n",
    "|                                   | `shape`                    | Check dimensions (rows, columns) of the DataFrame. |\n",
    "|                                   | `columns`                  | View the column names.                             |\n",
    "|                                   | `index`                    | View the row indexes.                              |\n",
    "| **Indexing and Selection**        | `loc[]`                    | Access rows/columns by labels or boolean conditions.|\n",
    "|                                   | `iloc[]`                   | Access rows/columns by integer-location indexing.  |\n",
    "|                                   | `at[]` / `iat[]`           | Access a single value by label/position.           |\n",
    "| **Data Cleaning and Handling**    | `isnull()`                 | Identify missing values.                           |\n",
    "|                                   | `notnull()`                | Identify non-missing values.                       |\n",
    "|                                   | `dropna()`                 | Remove missing values.                             |\n",
    "|                                   | `fillna()`                 | Fill missing values.                               |\n",
    "|                                   | `replace()`                | Replace specific values in the DataFrame.          |\n",
    "|                                   | `astype()`                 | Change the data type of a column.                  |\n",
    "| **Data Manipulation and Transformation** | `apply()`         | Apply a function along an axis of the DataFrame.   |\n",
    "|                                   | `map()`                    | Apply a function element-wise on a Series.         |\n",
    "|                                   | `applymap()`               | Apply a function element-wise on the DataFrame.    |\n",
    "|                                   | `assign()`                 | Add new columns or modify existing ones.           |\n",
    "|                                   | `rename()`                 | Rename columns or row indexes.                     |\n",
    "|                                   | `drop()`                   | Remove columns or rows.                            |\n",
    "| **Sorting and Ordering**          | `sort_values()`            | Sort by values along either axis.                  |\n",
    "|                                   | `sort_index()`             | Sort by the DataFrame index.                       |\n",
    "| **Aggregation and Grouping**      | `groupby()`                | Group data for aggregation.                        |\n",
    "|                                   | `agg()`                    | Aggregate data using different functions.          |\n",
    "|                                   | `pivot_table()`            | Create a pivot table from the data.                |\n",
    "| **Merging and Combining**         | `concat()`                 | Concatenate multiple DataFrames.                   |\n",
    "|                                   | `merge()`                  | Merge two DataFrames based on a common key.        |\n",
    "|                                   | `join()`                   | Join two DataFrames based on their indexes.        |\n",
    "| **Time Series Handling**          | `pd.to_datetime()`         | Convert a column to datetime format.               |\n",
    "|                                   | `resample()`               | Resample time series data.                         |\n",
    "|                                   | `shift()`                  | Shift index by a desired number of periods.        |\n",
    "| **Exporting Data**                | `to_csv()`                 | Export the DataFrame to a CSV file.                |\n",
    "|                                   | `to_excel()`               | Export the DataFrame to an Excel file.             |\n",
    "|                                   | `to_json()`                | Export the DataFrame to JSON format.               |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a Pandas Series\n",
    "s = pd.Series([100, 200, 300, 400], index=['A', 'B', 'C', 'D'])\n",
    "\n",
    "# Reaching elements in a serie\n",
    "print(s)\n",
    "print(s.loc[\"A\"])\n",
    "print(s.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a Pandas DataFrame\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 70000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Reaching elements in a dataframe\n",
    "print(df)\n",
    "print(\"--------------------\")\n",
    "print(df.loc[1])\n",
    "print(\"--------------------\")\n",
    "print(df.iloc[1])\n",
    "print(\"--------------------\")\n",
    "print(df.iloc[1][\"Name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "When working with data, we start by exploring it, checking the amount of data (number of rows), null values, range of dates, column types, etc. These are just examples of checks to perform before starting to manipulate the data. `Pandas` provides some tools to help with this. Let's discover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading csv file\n",
    "df_stocks = pd.read_csv(\"all_stocks_5yr.csv\")\n",
    "\n",
    "# Displaying samples of data\n",
    "display(df_stocks.head()) # displaying the first 5 rows\n",
    "display(df_stocks.tail(3)) # Displaying the last 3 rows\n",
    "print(df_stocks.dtypes)\n",
    "\n",
    "# Counting the number of rows\n",
    "print(\"##########################################################\")\n",
    "print(f\"The number of (rows, columns) in our dataframe: {df_stocks.shape}\")\n",
    "print(f\"The number of non null elements in every column: {df_stocks.count()}\")\n",
    "\n",
    "# Displaying statistical information about our data\n",
    "print(\"##########################################################\")\n",
    "display(df_stocks.describe())\n",
    "print(f\"The first date in the data: {df_stocks['date'].min()}\")\n",
    "print(f\"The last date in the data: {df_stocks['date'].max()}\")\n",
    "print(f\"The different stocks' names: {df_stocks['Name'].unique()}\")\n",
    "print(f\"The number of different stocks: {len(df_stocks['Name'].unique())}\")\n",
    "\n",
    "# Explore null values\n",
    "print(\"##########################################################\")\n",
    "for col in df_stocks.columns:\n",
    "    print(f\"The number of null values in the column {col}: {df_stocks[col].isnull().sum()}\") # since True is equivalent to 1 and False is equivalent to 0, when applying sum(), it counts the number of True values\n",
    "display(df_stocks[df_stocks.isnull().any(axis=1)]) # filter rows which any value == null (at least one null value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleansing\n",
    "After reading the data, we may apply some operations to clean it. The statements below are some examples:\n",
    "\n",
    "- The date column should be of the `datetime` type.\n",
    "- The analysis will be conducted from the beginning of 2014 until the end of 2017.\n",
    "- If the high price is missing, it should be filled with the maximum of the open and close prices.\n",
    "- If the low price is missing, it should be filled with the minimum of the open and close prices.\n",
    "- If the open price is missing, it should be filled with the close price of the previous day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The date column should be of the `datetime` type.\n",
    "df_stocks['date'] = pd.to_datetime(df_stocks['date'])\n",
    "\n",
    "# The analysis will be conducted from the beginning of 2014 until the end of 2017.\n",
    "df_stocks = df_stocks[(df_stocks[\"date\"].dt.year <= 2017) & (df_stocks[\"date\"].dt.year >= 2014)]\n",
    "\n",
    "\n",
    "# If the open price is missing, it should be filled with the close price of the previous day.\n",
    "df_stocks['close_lagged'] = (df_stocks.sort_values(by=['Name', 'date'], ascending=True)).groupby(['Name'])['close'].shift(1)\n",
    "df_stocks.loc[df_stocks['open'].isna(), 'open'] = df_stocks['close_lagged']\n",
    "del df_stocks['close_lagged']\n",
    "\n",
    "# If the high price is missing, it should be filled with the maximum of the open and close prices.\n",
    "df_stocks.loc[df_stocks['high'].isna(), 'high'] = np.fmax(df_stocks['close'], df_stocks[\"open\"]) # there is another function np.maximum that return the max of 2 values, but we didn't use it because it doesn't ignore Nan value\n",
    "\n",
    "# If the low price is missing, it should be filled with the minimum of the open and close prices.\n",
    "df_stocks.loc[df_stocks['low'].isna(), 'low'] = np.fmin(df_stocks['close'], df_stocks[\"open\"]) \n",
    "\n",
    "\n",
    "display(df_stocks[df_stocks.isnull().any(axis=1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "There are many other operations we can apply to a DataFrame for BI or machine learning purposes, such as generating new columns based on existing ones, grouping data to create an aggregated view, or joining it with other data sources to add more precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupby` operation can be summarized in three steps:\n",
    "\n",
    "1. **Splitting**: Dividing the data into groups based on some criteria (e.g., a specific column value).\n",
    "2. **Applying**: Applying a function or transformation to each group (e.g., calculating the mean, sum, count).\n",
    "3. **Combining**: Combining the results back into a single data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## display the mean of close price by YYYY-mm for the stock AAL\n",
    "\n",
    "# create a new column containing the year and the month YYYY-mm\n",
    "df_stocks['year_month'] = df_stocks['date'].dt.strftime('%Y-%m')\n",
    "# filter on AAL stock and group by the new column with computing the average of the column close\n",
    "mean_values = df_stocks[df_stocks[\"Name\"] == \"AAL\"].groupby('year_month')['close'].mean().reset_index()\n",
    "\n",
    "display(mean_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## display the mean of close price by YYYY-mm and the sum of the volume for each stock\n",
    "\n",
    "# create a new column containing the year and the month YYYY-mm\n",
    "df_stocks[\"year_month\"] = df_stocks[\"date\"].dt.strftime(\"%Y-%m\")\n",
    "# group by the new column with computing the average of the column close and the sum of the volume for each stock\n",
    "mean_sum_values = df_stocks.groupby([\"year_month\", \"Name\"]).agg({'close': 'mean', 'volume': 'sum'}).reset_index()\n",
    "\n",
    "display(mean_sum_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data analysis, it's common to work with multiple datasets that need to be combined to extract meaningful insights. Pandas provides powerful methods such as `merge` and `join` to perform these data junctions efficiently.\n",
    "\n",
    "Like SQL, Pandas supports different types of joins: `inner`, `outer`, `left`, and `right`. These allow you to control how data from multiple DataFrames is combined, based on matching keys.\n",
    "\n",
    "`merge` is a versatile method that allows you to join DataFrames based on one or more columns, offering flexibility for various data structures. In contrast, `join` is ideal for simpler cases where you want to combine data based on the DataFrame's index.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## joining the stocks dataframe to another one to have more information about the stocks\n",
    "\n",
    "df_stocks_info = pd.read_csv(\"stock_info.csv\")\n",
    "df_stocks_enriched = df_stocks.merge(df_stocks_info, how=\"left\", left_on=\"Name\", right_on=\"abbreviation\")\n",
    "display(df_stocks_enriched)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key concepts in windowing:\n",
    "\n",
    "1. **Partition**: (group) Divides the data into subsets, and the window function is applied separately to each partition.\n",
    "2. **Order**: Defines the sequence in which rows are processed within each partition.\n",
    "3. **Frame**: Specifies the range of rows within each partition that the window function should consider for calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing the moving average to have a smooth curve of close prices\n",
    "df_stocks_enriched['5_day_moving_avg'] = (df_stocks_enriched.sort_values(by=['Name', 'date'], ascending=True)).groupby('Name')['close'].transform(lambda x: x.rolling(window=5).mean())\n",
    "df_stocks_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "**Implementing Eigen Portfolios (PCA)**\n",
    "\n",
    "The goal here is to construct **Eigen Portfolios**.\n",
    "\n",
    "- Load the all_stocks_5yr.csv data into a Pandas dataframe.\n",
    "\n",
    "- Clean the dataframe from any null or nan values you encounter using linear interpolation. Be careful not to interpolate with another stock price, (hint: you may want to use groupby).\n",
    "\n",
    "- Reach the result from the example in the previous cell and construct the covariance matrix of all stocks available in the data using the smoothed out 5-day moving average prices.\n",
    "\n",
    "- Use what you have done in exercise 7 from the NumPy section to find the matrices P and D associated with the covariance matrix.\n",
    "Each vector in your matrix P is an eigen portfolio associated to the corresponding eigenvalue (variance) of your diagonal matrix D.\n",
    "Your first vector (ie. P[:,0]) is your first eigen portfolio, and it explains the most variance, your second vector (ie. P[:,1]) is your first eigen portfolio, and it explains the second most variance, etc...\n",
    "\n",
    "- Compute the realized return and variance of each eigen portfolio, what do you see ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"C4\">Introduction to Polars</a>\n",
    "\n",
    "Polars is a fast, multi-threaded DataFrame library designed for data analysis. It is written in Rust and provides Python bindings, making it accessible for Python developers. Polars offers high performance and efficiency, especially when working with large datasets.\n",
    "\n",
    "Unlike pandas, which operates mainly in a single-threaded manner, Polars takes full advantage of modern hardware by leveraging parallelism and multi-threading. This makes it a powerful choice for handling big data efficiently.\n",
    "\n",
    "We use `Polars` for its:\n",
    "- **Performance**: Polars is significantly faster than pandas, especially with large datasets, due to its multi-threaded architecture.\n",
    "- **Memory Efficiency**: Polars is designed to be more memory efficient, allowing you to work with larger datasets without running out of memory.\n",
    "- **Expressive Syntax**: Polars offers a modern and concise syntax, making data manipulation tasks easier to write and understand.\n",
    "- **Lazy Evaluation**: One of Polars' unique features is lazy evaluation, which optimizes data processing by only executing operations when needed, improving performance even further.\n",
    "\n",
    "\n",
    "| Feature            | pandas                       | Polars                         |\n",
    "|--------------------|------------------------------|--------------------------------|\n",
    "| Language           | Python (built on NumPy)      | Rust (with Python bindings)    |\n",
    "| Speed              | Single-threaded, slower on large datasets | Multi-threaded, optimized for performance |\n",
    "| DataFrame Immutability | Mutable (in-place modifications) | Immutable (creates new copies)|\n",
    "| Lazy Evaluation    | Not supported                | Supported (more efficient)     |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise**\n",
    "Replicate all the pandas' transformations using Polars."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
